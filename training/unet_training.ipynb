{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pytorch-unet-resnet18-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lcY1ziTLblo"
      },
      "source": [
        "## pytorch-unet\n",
        "\n",
        "https://github.com/usuyama/pytorch-unet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUvckFGU-4HE"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"pytorch_unet.py\"):\n",
        "  if not os.path.exists(\"pytorch_unet\"):\n",
        "    !git clone https://github.com/usuyama/pytorch-unet.git\n",
        "\n",
        "  %cd pytorch-unet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N90-BlegJZfs"
      },
      "source": [
        "## Enabling GPU on Colab\n",
        "\n",
        "Need to enable GPU from Notebook settings\n",
        "\n",
        "- Navigate to Edit-Notebook settings menu\n",
        "- Select GPU from the Hardware Accelerator dropdown list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCitpQdkJNdI"
      },
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"GPU not availalbe. CPU training will be too slow.\")\n",
        "\n",
        "print(\"device name\", torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg2FqLRGBEJT"
      },
      "source": [
        "## Prepare Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjJaa8ww_P9T"
      },
      "source": [
        "os.system('git clone https://github.com/VikramShenoy97/Human-Segmentation-Dataset.git')\n",
        "\n",
        "os.remove('Human-Segmentation-Dataset/Ground_Truth/.DS_Store')\n",
        "os.remove('Human-Segmentation-Dataset/Training_Images/.DS_Store')\n",
        "\n",
        "masks = os.listdir('Human-Segmentation-Dataset/Ground_Truth')\n",
        "print('Masks:', len(masks))\n",
        "\n",
        "imgs = os.listdir('Human-Segmentation-Dataset/Training_Images')\n",
        "print('Images:', len(imgs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-UTr03eAROb"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class PeopleDataset(object):\n",
        "\n",
        "    def __init__(self, root, transforms=(None, None)):\n",
        "\n",
        "        self.root = root\n",
        "        self.img_transform = transforms[0]\n",
        "        self.mask_transform = transforms[1]\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Training_Images\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"Ground_Truth\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = os.path.join(self.root, \"Training_Images\", self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        mask_path = os.path.join(self.root, \"Ground_Truth\", self.masks[idx])\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        if self.img_transform:\n",
        "            img = self.img_transform(img)\n",
        "        \n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask).numpy()\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "# use the same transformations for train/val in this example\n",
        "img_trans = transforms.Compose([\n",
        "  transforms.CenterCrop(224),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
        "])\n",
        "\n",
        "mask_trans = transforms.Compose([\n",
        "  transforms.CenterCrop(224),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = PeopleDataset('Human-Segmentation-Dataset', transforms = (img_trans, mask_trans))\n",
        "val_set = PeopleDataset('Human-Segmentation-Dataset', transforms = (img_trans, mask_trans))\n",
        "\n",
        "image_datasets = {\n",
        "  'train': train_set, 'val': val_set\n",
        "}\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "dataloaders = {\n",
        "  'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "  'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7XRZIKtCN8E"
      },
      "source": [
        "# Define a UNet module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8EJl0hcC5DH"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "\n",
        "def convrelu(in_channels, out_channels, kernel, padding):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
        "    nn.ReLU(inplace=True),\n",
        "  )\n",
        "\n",
        "\n",
        "class ResNetUNet(nn.Module):\n",
        "  def __init__(self, n_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.base_model = torchvision.models.resnet18(pretrained=True)\n",
        "    self.base_layers = list(self.base_model.children())\n",
        "\n",
        "    self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
        "    self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
        "    self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
        "    self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
        "    self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
        "    self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
        "    self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
        "    self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
        "    self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
        "    self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
        "\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
        "    self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
        "    self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
        "    self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
        "\n",
        "    self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
        "    self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
        "    self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
        "\n",
        "    self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x_original = self.conv_original_size0(input)\n",
        "    x_original = self.conv_original_size1(x_original)\n",
        "\n",
        "    layer0 = self.layer0(input)\n",
        "    layer1 = self.layer1(layer0)\n",
        "    layer2 = self.layer2(layer1)\n",
        "    layer3 = self.layer3(layer2)\n",
        "    layer4 = self.layer4(layer3)\n",
        "\n",
        "    layer4 = self.layer4_1x1(layer4)\n",
        "    x = self.upsample(layer4)\n",
        "    layer3 = self.layer3_1x1(layer3)\n",
        "    x = torch.cat([x, layer3], dim=1)\n",
        "    x = self.conv_up3(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    layer2 = self.layer2_1x1(layer2)\n",
        "    x = torch.cat([x, layer2], dim=1)\n",
        "    x = self.conv_up2(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    layer1 = self.layer1_1x1(layer1)\n",
        "    x = torch.cat([x, layer1], dim=1)\n",
        "    x = self.conv_up1(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    layer0 = self.layer0_1x1(layer0)\n",
        "    x = torch.cat([x, layer0], dim=1)\n",
        "    x = self.conv_up0(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    x = torch.cat([x, x_original], dim=1)\n",
        "    x = self.conv_original_size2(x)\n",
        "\n",
        "    out = self.conv_last(x)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7rAEQCUEI2v"
      },
      "source": [
        "# Define the main training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjt9JeTuDY6D"
      },
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from loss import dice_loss\n",
        "\n",
        "checkpoint_path = \"checkpoint.pth\"\n",
        "\n",
        "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "\n",
        "    pred = torch.sigmoid(pred)\n",
        "    dice = dice_loss(pred, target)\n",
        "\n",
        "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "\n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = calc_loss(outputs, labels, metrics)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print_metrics(metrics, epoch_samples, phase)\n",
        "            epoch_loss = metrics['loss'] / epoch_samples\n",
        "\n",
        "            if phase == 'train':\n",
        "              scheduler.step()\n",
        "              for param_group in optimizer.param_groups:\n",
        "                  print(\"LR\", param_group['lr'])\n",
        "\n",
        "            # save the model weights\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                print(f\"saving best model to {checkpoint_path}\")\n",
        "                best_loss = epoch_loss\n",
        "                torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcdAu9ZEOLG"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfxgL303EMiy"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device', device, '\\n')\n",
        "\n",
        "num_class = 1\n",
        "model = ResNetUNet(num_class).to(device)\n",
        "\n",
        "# freeze backbone layers\n",
        "for l in model.base_layers:\n",
        "  for param in l.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=8, gamma=0.1)\n",
        "\n",
        "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipNf9Sy5neVX"
      },
      "source": [
        "# Save model in dropbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf3V3ya1j0_7"
      },
      "source": [
        "!pip install dropbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_gyBWL6j24u"
      },
      "source": [
        "import dropbox\n",
        "access_token = \"YOUR_ACCESS_TOKEN\"\n",
        "dbx = dropbox.Dropbox(access_token, timeout=None)\n",
        "with open('checkpoint.pth', 'rb') as f:\n",
        "    dbx.files_upload(f.read(), '/people-segmenter-model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}